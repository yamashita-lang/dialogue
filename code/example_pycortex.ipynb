{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization Example using Pycortex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================\n",
      "               Random Search + Gradient Descent \n",
      "==================================================================\n"
     ]
    }
   ],
   "source": [
    "import cortex\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import os\n",
    "import tables\n",
    "\n",
    "# sys.path.append('/path/to/config.py')\n",
    "sys.path.append('/home/projects/dialogue/code/')\n",
    "from config import DATA_DIR, RESULT_DIR, SUBJECTS_ALL, TOTAL_SESS, LAYERS, CL, NVOXELS\n",
    "from code.utils_pycortex import (\n",
    "    get_volume_neg2nan_ALLSUBJ_base, get_volume_context_neg2nan_ALLSUBJ,\n",
    "    get_volume_cross_context_neg2nan_ALLSUBJ, get_volume2D_bestvp_neg2nan_ALLSUBJ,\n",
    "    get_volume2D_context_weightcorr_bothsignificant, get_volume_weightcorr_VP_mask,\n",
    "    get_volume2D_context_all_weightcorr_significant_unified_diff\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Head Motion model results\n",
    "head_motion_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step1_headmotion.hdf\"\n",
    "\n",
    "### Random embedding model results\n",
    "random_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step2_randnormal.hdf\"\n",
    "\n",
    "### Separate Linguistic model results\n",
    "separate_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_intramodal.hdf\"\n",
    "\n",
    "### Separate Linguistic model cross-modality prediction results\n",
    "cross_modality_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_crossmodal.hdf\"\n",
    "\n",
    "### Unified Linguistic model results\n",
    "unified_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_unified.hdf\"\n",
    "\n",
    "### Best variance partition results\n",
    "bestbp_file=f\"{RESULT_DIR}ALLSUBJ_best_varpart.hdf\"\n",
    "\n",
    "### Weight correlation in the Separate Linguistic model weights\n",
    "wc_sep_file=f\"{RESULT_DIR}ALLSUBJ_weightcorrs_gpt_separate.hdf\"\n",
    "\n",
    "### Weight correlation similarity between the Separate Linguistic model weights with Unified Linguistic model weights\n",
    "wc_unif_sep_file=f\"{RESULT_DIR}ALLSUBJ_weightcorrs_gpt_unified.hdf\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction accuracy (brain score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Motion model/Random embedding model (Supplementary Figs.3 and 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 29275\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "cmap='inferno'\n",
    "VMAX=0.3\n",
    "\n",
    "### files\n",
    "# head_motion_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step1_headmotion.hdf\"\n",
    "# random_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step2_randnormal.hdf\"\n",
    "\n",
    "volumes={}\n",
    "for subject_idx in range(8):\n",
    "    subject = SUBJECTS_ALL[subject_idx]\n",
    "\n",
    "    ### Head Motion model\n",
    "    fig_name=f\"{subject}_headmotion\"\n",
    "    if os.path.exists(head_motion_model_file):\n",
    "        volumes[fig_name]=get_volume_neg2nan_ALLSUBJ_base(subject_idx, subject, head_motion_model_file, cmap=cmap, vmax=VMAX)\n",
    "    else:\n",
    "        print(\"Not available {model_file}\")\n",
    "\n",
    "    ### Random embedding model\n",
    "    fig_name=f\"{subject}_random\"\n",
    "    if os.path.exists(random_model_file):\n",
    "        volumes[fig_name]=get_volume_neg2nan_ALLSUBJ_base(subject_idx, subject, random_model_file, cmap=cmap, vmax=VMAX)\n",
    "    else:\n",
    "        print(\"Not available {model_file}\")\n",
    "    \n",
    "im =cortex.webshow(volumes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separte and Unified Linguistic model (Fig 2a, Supplementary Figs 5-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 38127\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "cmap='hot'\n",
    "\n",
    "THR=0\n",
    "VMIN=0\n",
    "VMAX=0.3\n",
    "\n",
    "################################################################################################\n",
    "volumes={}\n",
    "for subject_idx in range(8):\n",
    "    subject = SUBJECTS_ALL[subject_idx]\n",
    "    for layer_idx, layer in enumerate(LAYERS):\n",
    "        if layer == 18:\n",
    "            for cl_idx, cl in enumerate(CL):\n",
    "                if cl in [1, 8, 32]:\n",
    "                    ### Separate Linguistic model \n",
    "                    # separate_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_intramodal.hdf\"\n",
    "                    # cross_modality_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_crossmodal.hdf\"\n",
    "                    # unified_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_unified.hdf\"\n",
    "                    test_idx=1\n",
    "                    if os.path.exists(separate_model_file):\n",
    "                        fig_name=f\"{subject}_Separete_L-{layer}_CL-{cl}\"\n",
    "                        volumes[fig_name]=get_volume_context_neg2nan_ALLSUBJ(subject_idx, subject, layer_idx, cl_idx, test_idx, separate_model_file, cmap=cmap, vmin=VMIN, vmax=VMAX, thr=THR)\n",
    "                    else:\n",
    "                        print(f\"Not available {separate_model_file}\")\n",
    "\n",
    "                    ### Separate Linguistic model cross-modality\n",
    "                    if os.path.exists(cross_modality_file):\n",
    "                        fig_name=f\"{subject}_Cross_L-{layer}_CL-{cl}\"\n",
    "                        volumes[fig_name]=get_volume_cross_context_neg2nan_ALLSUBJ(subject_idx, subject, layer_idx, cl_idx, cross_modality_file, separate_model_file, cmap=cmap, vmin=VMIN, vmax=VMAX, thr=THR)\n",
    "                    else:\n",
    "                        print(f\"Not available {cross_modality_file}\")\n",
    "\n",
    "                    ### Unified Linguistic model\n",
    "                    if os.path.exists(unified_model_file):\n",
    "                        fig_name=f\"{subject}_Unified_L-{layer}_CL-{cl}\"\n",
    "                        volumes[fig_name]=get_volume_context_neg2nan_ALLSUBJ(subject_idx, subject, layer_idx, cl_idx, test_idx, unified_model_file, cmap=cmap, vmin=VMIN, vmax=VMAX, thr=THR)\n",
    "                    else:\n",
    "                        print(f\"Not available {unified_model_file}\")\n",
    "                    \n",
    "\n",
    "im =cortex.webshow(volumes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best variance partitioning (Fig 4c, Supplementary Fig 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started server on port 29645\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "cmap=\"BROYG_2D\"\n",
    "\n",
    "# bestbp_file=f\"{RESULT_DIR}ALLSUBJ_best_varpart.hdf\"\n",
    "# separate_model_file=f\"{RESULT_DIR}ALLSUBJ_sig_brainscore_step3_gpt_intramodal.hdf\"\n",
    "THR=0.05\n",
    "VMAX=0.2\n",
    "\n",
    "volumes={}\n",
    "for subject_idx in range(8):\n",
    "    subject = SUBJECTS_ALL[subject_idx]    \n",
    "    for layer_idx, layer in enumerate(LAYERS):\n",
    "        if layer == 18:\n",
    "            for cl_idx, cl in enumerate(CL):\n",
    "                if cl in [1, 8, 32]:\n",
    "                    fig_name=f\"{subject}_l-{layer}_s-{cl}\"\n",
    "                    volumes[fig_name]=get_volume2D_bestvp_neg2nan_ALLSUBJ(subject_idx, subject, layer_idx, cl_idx, bestbp_file, separate_model_file, vmin=THR, vmax=VMAX, thr=THR, cmap=cmap)\n",
    "\n",
    "im =cortex.webshow(volumes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WC in cross-modal voxels, bimodal voxels, and differences (Fig 2c/SFig 13, 3d/SFig 14, 4e/SFig 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of prod, comp, bimodal voxels: (23400, 18908, 24659)\n",
      "# of prod, comp, bimodal voxels: (25728, 20887, 18732)\n",
      "# of prod, comp, bimodal voxels: (19385, 17806, 28672)\n",
      "# of prod, comp, bimodal voxels: (25203, 20087, 21282)\n",
      "# of prod, comp, bimodal voxels: (28382, 14295, 23177)\n",
      "# of prod, comp, bimodal voxels: (18436, 21497, 23231)\n",
      "# of prod, comp, bimodal voxels: (29865, 14066, 18888)\n",
      "# of prod, comp, bimodal voxels: (16684, 18445, 26945)\n",
      "# of prod, comp, bimodal voxels: (17616, 20106, 23528)\n",
      "# of prod, comp, bimodal voxels: (26633, 15996, 17929)\n",
      "# of prod, comp, bimodal voxels: (14212, 23827, 21917)\n",
      "# of prod, comp, bimodal voxels: (10455, 26676, 21216)\n",
      "# of prod, comp, bimodal voxels: (21427, 20442, 22779)\n",
      "# of prod, comp, bimodal voxels: (14379, 29095, 20715)\n",
      "# of prod, comp, bimodal voxels: (15351, 27679, 20109)\n",
      "# of prod, comp, bimodal voxels: (22442, 20590, 24644)\n",
      "# of prod, comp, bimodal voxels: (15816, 28302, 22404)\n",
      "# of prod, comp, bimodal voxels: (11033, 32782, 21005)\n",
      "# of prod, comp, bimodal voxels: (31384, 13831, 16797)\n",
      "# of prod, comp, bimodal voxels: (29481, 11558, 20450)\n",
      "# of prod, comp, bimodal voxels: (14059, 15132, 30914)\n",
      "# of prod, comp, bimodal voxels: (25538, 18977, 24825)\n",
      "# of prod, comp, bimodal voxels: (25439, 18423, 23201)\n",
      "# of prod, comp, bimodal voxels: (18169, 21572, 25990)\n",
      "Started server on port 32641\n",
      "Stopping server\n"
     ]
    }
   ],
   "source": [
    "### parameters\n",
    "TARGET_LAYER=18\n",
    "\n",
    "##########################################################################################\n",
    "volumes={}\n",
    "for subject_idx in range(8):\n",
    "    subject = SUBJECTS_ALL[subject_idx]\n",
    "    for cl_idx, cl in enumerate(CL):\n",
    "        if cl in [1, 8, 32]:\n",
    "            for layer_idx, layer in enumerate(LAYERS):\n",
    "                if layer == TARGET_LAYER:\n",
    "                    ### cross-modal voxels\n",
    "                    ## Fig. 2c WC in cross-modal voxels (Supplementary Fig 13)\n",
    "                    fig_name=f\"{subject}_l-{layer}_{cl}_crossmodal_voxels\"\n",
    "                    cmap=\"BuWtRd_alpha\"\n",
    "                    WC_MAX=0.4\n",
    "                    SCORE_MIN=0.05\n",
    "                    SCORE_MAX=0.2\n",
    "                    volumes[fig_name] = get_volume2D_context_weightcorr_bothsignificant(subject, subject_idx, layer_idx, cl_idx, wc_sep_file, separate_model_file, cross_modality_file, \n",
    "                                                                                vmin=-WC_MAX, vmax=WC_MAX, score_min=SCORE_MIN, score_max=SCORE_MAX, cmap=cmap)\n",
    "\n",
    "                    ### bimodal voxels\n",
    "                    modality_idx=3\n",
    "                    ## Fig. 4e WC in bimodal voxels (Supplementary Fig 16)\n",
    "                    fig_name=f\"{subject}_l-{layer}_{cl}_bimodal_voxels\"\n",
    "                    volumes[fig_name] = get_volume_weightcorr_VP_mask(subject, subject_idx, layer_idx, cl_idx, modality_idx, wc_sep_file, bestbp_file, separate_model_file, \n",
    "                                                                                vmin=-WC_MAX, vmax=WC_MAX, cmap=cmap)\n",
    "\n",
    "                    \n",
    "                    ### wc differences \n",
    "                    ## Fig 3d WC difference between Unified model weights and Separate model weights (comp - prod) in unified scores (Supplementary Fig 14)\n",
    "                    fig_name=f\"{subject}_l-{layer}_{cl}_diff\"\n",
    "                    cmap=\"BuBkRd_alpha_2D\"\n",
    "                    WC_MAX=0.3\n",
    "                    SCORE_MIN=0.05\n",
    "                    SCORE_MAX=0.3\n",
    "                    volumes[fig_name] = get_volume2D_context_all_weightcorr_significant_unified_diff(subject, subject_idx, layer_idx, cl_idx, wc_unif_sep_file, separate_model_file, unified_model_file, \n",
    "                                                                                vmin=-WC_MAX, vmax=WC_MAX, score_min=SCORE_MIN, score_max=SCORE_MAX, cmap=cmap)\n",
    "\n",
    "im =cortex.webshow(volumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
